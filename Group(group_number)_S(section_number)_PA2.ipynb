{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10946804,"sourceType":"datasetVersion","datasetId":6808833},{"sourceId":10977342,"sourceType":"datasetVersion","datasetId":6830905},{"sourceId":10992677,"sourceType":"datasetVersion","datasetId":6842312}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/27100340/ai-ml-pa2/blob/main/Group(group_number)_S(section_number)_PA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"<center>\n\n# **PA2: Classification**\n\n### **Total : 1000 marks**\n\n</center>\n\n**Group Number:**\n\nName:\n\nRoll Number:\n\nName:\n\nRoll Number:\n","metadata":{"id":"tjsfpygDxxzs"}},{"cell_type":"markdown","source":"## Instructions\n\n- Follow along with the notebook, filling out the necessary code where instructed.\n\n- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n\n- <span style=\"color: red;\">Do not remove any pre-written code (unless explicitly mentioned in the cell that you are allowed to do so).</span>\n\n- <span style=\"color: red;\">You must attempt all parts.</span>\n\n## Submission Guidelines\n\nYou are to submit a zip file containing the following files:\n\n1. This juptyer notebook. `Group<group_number>_S<section_number>_PA2.ipynb`\n2. Python file of the juptyer notebook. `Group<group_number>_S<section_number>_PA2.py`\n\nFor example if I am from S1 and my group number is 25, my files would be named `Group25_S1_PA2.ipynb` and `Group25_S1_PA2.py`\n\nIMPORTANT: **Do not zip the files.** Submit both the files seperately, without zipping them.\n","metadata":{"id":"7LVX_zdHxxzw"}},{"cell_type":"markdown","source":"# Part 1: k-NNs from Scratch (400 marks)\n\nYou are <span style=\"color: red;\">not allowed</span> to use scikit-learn or any other machine learning toolkit for this part. You have to implement your own k-NN classifier from scratch.\n\n### Importing Libraries\n\nAll of the necessary libraries for this part have been imported for you below. You may not use any other library apart from standard Python librares.\n","metadata":{"id":"JnY_xh61xxzx"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom tqdm import tqdm\nimport PIL\n!pip install idx2numpy\n!pip install tqdm\nfrom tqdm.notebook import tqdm\nimport idx2numpy\nimport cupy as cp\nimport time\n# hellop","metadata":{"id":"0GJXkcCaxxzy","outputId":"48f4ddf5-16c0-4f6a-d748-2e1555b7d698","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:57:35.529024Z","iopub.execute_input":"2025-03-11T08:57:35.529399Z","iopub.status.idle":"2025-03-11T08:57:44.878523Z","shell.execute_reply.started":"2025-03-11T08:57:35.529363Z","shell.execute_reply":"2025-03-11T08:57:44.877530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 1.1: Extracting the dataset\n\nThe MNIST dataset consists of 70,000 labeled images of handwritten digits, each of size 28 pixels by 28 pixels, yielding a total of 784 pixels per picture.\n\nEach pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This value ranges from 0-255\n\nThe dataset can be downloaded from [here](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) and is also available to in your assignment directory. The four relevant files in the folder are:\n\n- train-images-idx3-ubyte: training set images\n- train-labels-idx1-ubyte: training set labels\n- t10k-images-idx3-ubyte: test set images\n- t10k-labels-idx1-ubyte: test set labels\n\nThe dataset has been split with 60,000 images in the train set, and the remaning 10,000 images in the test set.\n\nYour very first task is to to convert this dataset into a pandas dataframe.\n\nHint: _use the idx2numpy package to convert the dataset to a multidimensional numpy array. The documentation can be visited [here](https://pypi.org/project/idx2numpy/). The resulting array then has to be flattened._\n","metadata":{"id":"YlGZFk1vxxz0"}},{"cell_type":"code","source":"# Input the file paths\ntrain_images_path = \"/kaggle/input/mnistt/train-images.idx3-ubyte\"\ntrain_labels_path = \"/kaggle/input/mnistt/train-labels.idx1-ubyte\"\ntest_images_path = \"/kaggle/input/mnistt/t10k-images.idx3-ubyte\"\ntest_labels_path = \"/kaggle/input/mnistt/t10k-labels.idx1-ubyte\"\n\n# Convert the idx files to NumPy\ntrain_set_images = idx2numpy.convert_from_file(train_images_path)\ntrain_set_labels = idx2numpy.convert_from_file(train_labels_path)\ntest_set_images = idx2numpy.convert_from_file(test_images_path)\ntest_set_labels = idx2numpy.convert_from_file(test_labels_path)\n\n# Convert NumPy arrays to CuPy arrays\ntrain_set_images = cp.array(train_set_images)\ntrain_set_labels = cp.array(train_set_labels)\ntest_set_images = cp.array(test_set_images)\ntest_set_labels = cp.array(test_set_labels)\n\n# Print the shape of the arrays directly (CuPy supports .shape)\nprint(f\"Training images array shape: {train_set_images.shape}\")\nprint(f\"Test images array shape: {test_set_images.shape}\")","metadata":{"id":"q7aMaWLaxxz1","outputId":"3bbca5b7-84fd-4319-e87f-07ab0a5fb386","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:57:50.839685Z","iopub.execute_input":"2025-03-11T08:57:50.840004Z","iopub.status.idle":"2025-03-11T08:57:51.403672Z","shell.execute_reply.started":"2025-03-11T08:57:50.839979Z","shell.execute_reply":"2025-03-11T08:57:51.402919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: Flatten the array, append the labels and print the shape again\n# The shape of your train set should be (60000, 785), and test set should be (10000, 785)\n# IMPORTANT: If your shapes are not matching, dont attempt the rest of the assignment unless you get it right.\n# Ensure data is in CuPy format before using cp.column_stack\n# Flatten images (reshape while keeping them as CuPy arrays)\ntrain_set_images_flatten = train_set_images.reshape(60000, -1)\ntest_set_images_flatten = test_set_images.reshape(10000, -1)\n\n# Ensure labels are column vectors for stacking\ntrain_set_labels = train_set_labels.reshape(-1, 1)\ntest_set_labels = test_set_labels.reshape(-1, 1)\n\n# Stack images and labels using CuPy\ntrain_with_labels = cp.column_stack((train_set_images_flatten, train_set_labels))\ntest_with_labels = cp.column_stack((test_set_images_flatten, test_set_labels))\n\n# Print shapes (convert back to NumPy only for printing)\nprint(f\"train_with_labels.shape: {train_with_labels.shape}\")\nprint(f\"test_with_labels.shape: {test_with_labels.shape}\")\n","metadata":{"id":"tFMvCojLxxz2","outputId":"c382ca9a-aa45-4b51-9758-001ba93861e5","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:57:58.605064Z","iopub.execute_input":"2025-03-11T08:57:58.605373Z","iopub.status.idle":"2025-03-11T08:57:58.810559Z","shell.execute_reply.started":"2025-03-11T08:57:58.605348Z","shell.execute_reply":"2025-03-11T08:57:58.809632Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Q. What does each row of the dataset represents?\n","metadata":{"id":"ORB1ttxCxxz3"}},{"cell_type":"markdown","source":"Ans.\n","metadata":{"id":"gYvnT38Vxxz3"}},{"cell_type":"markdown","source":"### Task 1.2: Visualizing and preprocessing the dataset\n\nNow that we have a dataset to work with, we need to preprocess it further, before we pass it through our classifier. In this step, we will be seperating out the labels from the inputs, and attempt to standardize or normalize our dataset.\n\nNote that the standardization of a variable $x$ refers to:\n\n$$\nx' = \\frac{x - μ}{σ}\n$$\n\nwhere $μ$ is the mean of the variable and $σ$ is the standard deviation.\n\nOn the other hand, variable normalization usually involves scaling the data to a specific range.\n\nYou can read more about this [here](https://www.simplilearn.com/normalization-vs-standardization-article).\n\nAfter you've loaded and split the dataset, let's display some images. You can reshape these 784 values for each image, into a `28x28` array, then use either `matplotlib` or `PIL` to display the image.\n","metadata":{"id":"vV2FMINNxxz4"}},{"cell_type":"code","source":"# TODO: Extract labels and features\ntrain_x = train_with_labels[:, :-1]\ntrain_y = train_with_labels[:, -1]\ntest_x = test_with_labels[:, :-1]\ntest_y = test_with_labels[:, -1]\n\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)\n","metadata":{"id":"Vu0VqYCvxxz5","outputId":"6f0626cc-04d6-44be-9b3b-1ce4510f90df","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:58:03.102262Z","iopub.execute_input":"2025-03-11T08:58:03.102579Z","iopub.status.idle":"2025-03-11T08:58:03.108155Z","shell.execute_reply.started":"2025-03-11T08:58:03.102550Z","shell.execute_reply":"2025-03-11T08:58:03.107357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: Implement a function to display image. The label should be used as a title\n#      Randomly pick 5 rows from the training dataset and display their images\nrow_test = train_x[10, :]\nrow_test_label = train_y[10]\n\ndef display_image(features, label):\n    '''\n      Takes a 1D numpy array, reshapes to a 28x28 array and displays the image\n    '''\n    image = cp.asnumpy(features).reshape(28, 28)  # Convert CuPy to NumPy for plotting\n    plt.imshow(image, cmap='gray')\n    plt.title(f\"{cp.asnumpy(label)}\")\n    plt.axis('off')\n    plt.show()\n    return None\n\ndisplay_image(row_test, row_test_label)\n\nrandom_index = cp.random.choice(train_x.shape[0], 5, replace=False)\nfor i in random_index:\n    display_image(train_x[i, :], train_y[i])","metadata":{"id":"58YF8dIQxxz6","outputId":"88529da0-7e3a-4a9b-d509-622c320c3fba","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:58:07.236731Z","iopub.execute_input":"2025-03-11T08:58:07.237022Z","iopub.status.idle":"2025-03-11T08:58:08.698224Z","shell.execute_reply.started":"2025-03-11T08:58:07.237000Z","shell.execute_reply":"2025-03-11T08:58:08.697471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: normalize the data so that it falls in the [0, 1] range.\n#      Hint: the max value of each pixel is 255\nprint(\"Type of train_x before normalization:\", type(train_x))\ndef normalize(data):\n    '''\n      scales the data to the range [0, 1]\n    '''\n    return data / 255.0\n\ntrain_x = normalize(train_x)\ntest_x = normalize(test_x)\n\nprint(train_x.shape)\nprint(test_x.shape)","metadata":{"id":"PcpUfvWzxxz6","outputId":"bb0b8ea8-d523-4931-b722-ba4f14afba9c","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:58:17.331283Z","iopub.execute_input":"2025-03-11T08:58:17.331592Z","iopub.status.idle":"2025-03-11T08:58:17.525017Z","shell.execute_reply.started":"2025-03-11T08:58:17.331568Z","shell.execute_reply":"2025-03-11T08:58:17.524285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Q. With the variable standardization formula shown above in the description, is this technique feasible in this dataset? Explain in detail.\n","metadata":{"id":"aTKfYUKgxxz7"}},{"cell_type":"markdown","source":"Ans.\n","metadata":{"id":"y8D3EGV7xxz7"}},{"cell_type":"markdown","source":"### Task 1.3: Implementing k-NN Classifier\n\nNow you can create your own k-NN classifier. You can use the following steps as a guide:\n\n1. For a test data point, find its distance from all training instances.\n\n2. Sort the calculated distances in ascending order based on distance values.\n\n3. Choose k training samples with minimum distances from the test data point.\n\n4. Return the _most frequent_ class of these samples.\n\nFor values of `k` where a tie occurs, you need to break the tie by backing off to the `k-1` value. In case there is still a tie, you will continue decreasing `k` until there is a clear winner.\n\n#### Important\n\n**Note:** Your function should work with _Euclidean_ distance as well as _Manhattan_ distance. Pass the distance metric as a parameter in the k-NN classifier function. Your function should also let one specify the value of `k`.\n\n**Note:** Your approach should be vectorized. Failure to implement a vectorization-based method to calculate distances will result in significant loss of marks. You can read up vectorization [here](https://towardsdatascience.com/vectorization-implementation-in-machine-learning-ca652920c55d)\n\n#### Distance functions\n\nImplement separate functions for the Euclidean and Manhattan distances. Formulas for both are given below.\n\n$$\nd_{\\text{Euclidean}}(\\vec{p},\\vec{q}) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + (p_3 - q_3)^2 + ... + (p_n - q_n)^2}\n$$\n\n$$\nd_{\\text{Manhattan}}(\\vec{p},\\vec{q}) = |(p_1 - q_1)| + |(p_2 - q_2)| + |(p_3 - q_3)| + ... + |(p_n - q_n)|\n$$\n\n---\n\nComplete the following method functions:\n\n- `euclidean_distance`\n- `manhattan_distance`\n- `fit`\n- `get_neighbors`\n- `predict`\n","metadata":{"id":"yuJKdnFxxxz7"}},{"cell_type":"code","source":"class KNN:\n    def __init__(self, k):\n        '''\n        Initializes the class.\n        '''\n        self.k = k\n        self.train_x = None\n        self.train_y = None\n\n    def euclidean_distance(self, x1, x2):\n        '''\n        Takes two cupy arrays and calculates the Euclidean distance between them.\n        x1: single test point (shape: (784,))\n        x2: training set (shape: (n_samples, 784))\n        '''\n        # print(\"Calculating Euclidean distances...\")\n        euc_dist_square = cp.sum((x1 - x2) ** 2, axis=1)\n        euc_dist = cp.sqrt(euc_dist_square)\n        # print(\"Done!\\n\")\n        return euc_dist\n\n    def manhattan_distance(self, x1, x2):\n        '''\n        Takes two cupy arrays and calculates the Manhattan distance between them.\n        x1: single test point (shape: (784,))\n        x2: training set (shape: (n_samples, 784))\n        '''\n        # print(\"Calculating Manhattan distances...\\n\")\n        man_dist1 = cp.abs(x1 - x2)\n        man_res = cp.sum(man_dist1, axis=1)\n        # print(\"Done!\\n\")\n        return man_res\n\n    def fit(self, train_x, train_y):\n        '''\n        Stores the training dataset.\n        '''\n        self.train_x = train_x\n        self.train_y = train_y\n        return None\n\n    def get_neighbors(self, new_point, distancefunc):\n        '''\n        Takes a new point and returns the k nearest neighbors.\n        '''\n        if distancefunc == 'euclidean':\n            distances = self.euclidean_distance(new_point, self.train_x)\n        else:\n            distances = self.manhattan_distance(new_point, self.train_x)\n\n        sort = cp.argsort(distances)\n        nk_index = sort[:self.k]\n        nk_label = self.train_y[nk_index]\n        return nk_label\n\n    def predict(self, test_x, distancefunc):\n        '''\n        Takes a test set and returns the predicted labels.\n        Displays a progress bar for the prediction process.\n        '''\n        predictions = []\n        for i in tqdm(range(test_x.shape[0]), desc=\"Predicting\"):\n            k_val = self.k\n            prediction = None\n            while k_val > 0:\n                if distancefunc == 'euclidean':\n                    distances = self.euclidean_distance(test_x[i], self.train_x)\n                else:\n                    distances = self.manhattan_distance(test_x[i], self.train_x)\n\n                sorted_indices = cp.argsort(distances)\n                neighbor_indices = sorted_indices[:k_val]\n                neighbor_labels = self.train_y[neighbor_indices]\n                votes, counts = cp.unique(neighbor_labels, return_counts=True)\n\n                if cp.sum(counts == cp.max(counts)) == 1:  # No tie\n                    prediction = votes[cp.argmax(counts)]\n                    break\n                else:\n                    k_val -= 1  # Reduce k if tie\n\n            if prediction is None:  # Fallback if all k values tie\n                nns = self.get_neighbors(test_x[i], distancefunc)\n                votes, counts = cp.unique(nns, return_counts=True)\n                prediction = votes[cp.argmax(counts)]\n            predictions.append(prediction)\n        return cp.array(predictions)","metadata":{"id":"ihHwVjz_xxz8","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:58:23.224877Z","iopub.execute_input":"2025-03-11T08:58:23.225210Z","iopub.status.idle":"2025-03-11T08:58:23.234257Z","shell.execute_reply.started":"2025-03-11T08:58:23.225182Z","shell.execute_reply":"2025-03-11T08:58:23.233529Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 1.4: Evaluation\n\nNow that you've created a model and \"trained\" it, you can move on to the Evaluation phase.\n\n- We will be implementing an `evaluate` function that computes the Confusion Matrix, Accuracy, and Macro-Average F1 score of your classifier. You can use multiple helper functions to calculate the individual metrics.\n\n- The function should take as input the predicted labels and the true labels. This will be built in steps: its easier to create a Confusion Matrix, then calculate things like the Precision, Recall and F1 from it.\n\n- We will also implement a function that displays our confusion matrix as a heatmap annotated with the data values.\n- The axes should be properly labelled and the colormap used needs to be shown next to the heatmap.\n- You can have a look at some examples of heatmaps [here](https://seaborn.pydata.org/generated/seaborn.heatmap.html). (You don't have to use the seaborn libray, but it has some pretty colour palettes to choose from.)\n\nWe recommend that you do not use hard coding in this function.\n\n---\n\nComplete the following functions:\n\n- `accuracy`\n- `make_confusion_matrix`\n- `make_heat_map`\n- `precision`\n- `recall`\n- `f1_score`\n- `macro_average_f1`\n- `evaluate`\n","metadata":{"id":"DAihOg6axxz9"}},{"cell_type":"code","source":"def make_confusion_matrix(predicted_labels, true_labels):\n    '''\n      Takes the predicted labels and the true labels and returns the confusion matrix\n      Hint: You can create a helper function which calculates each row of the confusion matrix\n    '''\n    predicted_labels = cp.asarray(predicted_labels, dtype=cp.int32)\n    true_labels = cp.asarray(true_labels, dtype=cp.int32)\n    classes = int(cp.max(cp.concatenate((true_labels, predicted_labels))) + 1)\n    conf_matrix = cp.zeros((classes, classes), dtype=cp.int32)\n    cp.add.at(conf_matrix, (true_labels, predicted_labels), 1)\n    return conf_matrix\n\n# helper\ndef accuracy_helper(conf_matrix):\n    total = cp.sum(conf_matrix)\n    TP = cp.trace(conf_matrix)\n    FN = total - TP\n    TN = 0  # Not used in multi-class accuracy\n    FP = 0  # Not used in multi-class accuracy\n    return TN, TP, FN, FP\n\ndef accuracy(predicted_labels, true_labels):\n    '''\n      Takes the predicted labels and the true labels and returns the accuracy\n    '''\n    conf_matrix = make_confusion_matrix(predicted_labels, true_labels)\n    correct = cp.trace(conf_matrix)  # Sum of diagonal (correct predictions)\n    total = cp.sum(conf_matrix)      # Total number of predictions\n    return correct / total\n\ndef precision(confusion_matrix, class_label):\n    '''\n      Takes the confusion matrix and a label and returns the precision\n    '''\n    tp = confusion_matrix[class_label, class_label]\n    fp = cp.sum(confusion_matrix[:, class_label]) - tp\n    return tp / (tp + fp) if (tp + fp) != 0 else 0\n\ndef recall(confusion_matrix, class_label):\n    '''\n      Takes the confusion matrix and a label and returns the recall\n    '''\n    tp = confusion_matrix[class_label, class_label]\n    fn = cp.sum(confusion_matrix[class_label, :]) - tp\n    return tp / (tp + fn) if (tp + fn) != 0 else 0\n\ndef f1_score(precision, recall):\n    '''\n      Takes the precision and recall and returns the f1 score\n    '''\n    if precision + recall == 0:\n        return 0\n    return 2 * (precision * recall) / (precision + recall)\n\ndef macro_average_f1(confusion_matrix):\n    '''\n    Calculates the macro-average F1 score from a provided confusion matrix, over all classes\n    '''\n    num_classes = confusion_matrix.shape[0]\n    f1_scores = []\n    for class_label in range(num_classes):\n        prec = precision(confusion_matrix, class_label)\n        rec = recall(confusion_matrix, class_label)\n        f1 = f1_score(prec, rec)\n        f1_scores.append(f1)  # f1 is a CuPy scalar\n    return cp.mean(cp.array(f1_scores))  # Convert list to CuPy array and compute mean\n\ndef make_heat_map(confusion_matrix, title):\n    '''\n      Takes the confusion matrix and plots it as a heatmap\n    '''\n    # This function has already been implemented\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(cp.asnumpy(confusion_matrix), annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(title)\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.show()\n    return None\n\ndef evaluate(predicted_labels, true_labels):\n    '''\n      Displays and returns a nicely formatted report with accuracy, macro-average f1 score, and confusion matrix\n    '''\n    conf_matrix = make_confusion_matrix(predicted_labels, true_labels)\n    acc = accuracy(predicted_labels, true_labels)\n    macro_f1 = macro_average_f1(conf_matrix)\n    # Convert CuPy scalars to Python scalars for printing\n    acc_val = cp.asnumpy(acc).item()\n    macro_f1_val = cp.asnumpy(macro_f1).item()\n    print(\"Evaluation Results:\\n\")\n    print(f\"Accuracy: {acc_val}\")\n    print(f\"Macro-Average F1 Score: {macro_f1_val}\")\n    print(f\"Confusion Matrix:\\n{cp.asnumpy(conf_matrix)}\")\n    make_heat_map(conf_matrix, \"Heat Map\")\n    return acc, macro_f1","metadata":{"id":"1a6yugbExxz9","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:58:28.491298Z","iopub.execute_input":"2025-03-11T08:58:28.491605Z","iopub.status.idle":"2025-03-11T08:58:28.502709Z","shell.execute_reply.started":"2025-03-11T08:58:28.491580Z","shell.execute_reply":"2025-03-11T08:58:28.501793Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 1.5: k-fold Cross Validation\n\n<center>\n    <img src=\"https://global.discourse-cdn.com/dlai/original/3X/a/3/a3ed2de61c2b4fa00f1b7e939753e1a7e181afb0.png\">\n</center>\n\nNow with the basics done, you can move on to the next step: k-fold Cross Validation. This is a more robust way of evaluating your model since it uses all the data for training and testing (effectively giving you `k` chances to verify the generalizability of your model).\n\nNow, implement a function that performs `k`-fold cross-validation on the training data for a specified value of `k`.\n\nIn Cross Validation, you divide the dataset into `k` parts. `k-1` parts will be used for training and `1` part will be used for validation. You will repeat this process `k` times, each time using a different part for validation. You will then average the results of each fold to get the final result. Take a look at the image above for a better understanding.\n\nThe function should return **predictions** for the **entire training data** (size of list/array should be equal to the size of the dataset). This is the result of appending the predicted labels for each validation-train split into a single list/array. Make sure the order of the predicted labels matches the order of the training dataset, so that they may directly be passed to your `evaluate` function together with the actual labels.\n\n---\n\nComplete the following functions:\n\n- `k_fold_split`\n- `k_fold_cross_validation`\n","metadata":{"id":"mXrr_bQmxxz-"}},{"cell_type":"code","source":"def k_fold_split(num_folds, cv_no, train_x, train_y):\n    '''\n    Creates the train and test splits based off the value of k\n\n    Parameters\n    ----------\n    mum_folds : int\n        Number of folds\n    cv_no : int\n        The current fold number\n    train_x : nparray\n        The features\n    train_y : nparray\n        The labels\n    '''\n\n    total_samples = train_x.shape[0]\n    fold_size = total_samples // num_folds\n    start_val = (cv_no - 1) * fold_size\n    end_val = cv_no * fold_size if cv_no < num_folds else total_samples\n\n    validation_indexes = cp.arange(start_val, end_val)\n    train_indexes = cp.concatenate([cp.arange(0, start_val), cp.arange(end_val, total_samples)])\n\n    train_part_x = train_x[train_indexes]\n    train_part_y = train_y[train_indexes]\n    validate_x = train_x[validation_indexes]\n    validate_y = train_y[validation_indexes]\n\n    return train_part_x, train_part_y, validate_x, validate_y, validation_indexes\n\n\ndef k_fold_cross_validation(num_folds, k, train_x, train_y, distanceFunction):\n    \"\"\"\n    Returns the predictions for all the data points in the dataset using k-fold cross validation\n\n    num_folds: int\n      Number of folds\n    k: int\n      Number of neighbours to consider (hyperparameter)\n    train_x : nparray\n        The features\n    train_y : nparray\n        The labels\n    distanceFunction : str\n        Distance metric specified (manhattan / euclidean)\n    \"\"\"\n    all_predictions = []\n    for cv_no in range(1, num_folds + 1):\n        train_part_x, train_part_y, validate_x, validate_y, validation_indexes = k_fold_split(num_folds, cv_no, train_x, train_y)\n        model = KNN(k)\n        model.fit(train_part_x, train_part_y)\n        \n        predictions = model.predict(validate_x, distanceFunction)\n        all_predictions.append(predictions)\n    return cp.concatenate(all_predictions)\n","metadata":{"id":"QLWaX77kxxz-","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:58:36.105701Z","iopub.execute_input":"2025-03-11T08:58:36.106025Z","iopub.status.idle":"2025-03-11T08:58:36.112676Z","shell.execute_reply.started":"2025-03-11T08:58:36.105998Z","shell.execute_reply":"2025-03-11T08:58:36.111890Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now run your cross-validation function on the training data using `5-fold cross validation` for the values of `k = [1, 2, 3, 4, 5]`.\n\nDo this for both the Euclidean distance and the Manhattan distance for each value of `k`.\n\nAlso run your evaluation function for each value of `k` (for both distance metrics) and print out the classification accuracy and F1 score.\n\n**Note: Save your evaluation stats for plotting later**\n","metadata":{"id":"mjJjwyPAxxz_"}},{"cell_type":"code","source":"# Save scores here for plotting later\n# accuracy_list_euclidean = []\n# f1_list_euclidean = []\n# accuracy_list_manhattan = []\n# f1_list_manhattan = []\n\n# For K fold cross validation you can randomly sample a subset of training dataset to conduct your cross validation on.\n# The size of this dataset should be 14000.\n# This should significantly cut down the run time.\n# This has already been implemented for you in this cell\n# Now use sampled_train_x and sampled_train_y in the next cell\n\naccuracy_list_euclidean = []\nf1_list_euclidean = []\naccuracy_list_manhattan = []\nf1_list_manhattan = []\n\n\ndata = cp.hstack((train_x, train_y.reshape(-1, 1)))\nsampled_data = data[cp.random.choice(data.shape[0], 14000, replace=False)]\nsampled_train_x = sampled_data[:, :-1]\nsampled_train_y = sampled_data[:, -1]\n\nprint(sampled_train_x)\nprint(sampled_train_y)","metadata":{"id":"6ABnUkVJxx0A","outputId":"8d987888-6da7-4c15-c9ed-2ab0276bdae9","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:58:43.060816Z","iopub.execute_input":"2025-03-11T08:58:43.061159Z","iopub.status.idle":"2025-03-11T08:58:43.457200Z","shell.execute_reply.started":"2025-03-11T08:58:43.061126Z","shell.execute_reply":"2025-03-11T08:58:43.456418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: Perform cross-validation for both distances and run your evaluation function for each K, printing the accuracy and macro-average F1 score.\n\nnum_folds = 5\nk_values = [1, 2, 3, 4, 5]\n\naccuracy_list_euclidean = {}\nf1_list_euclidean = {}\naccuracy_list_manhattan = {}\nf1_list_manhattan = {}\n\nprint(\"Manhattan DISTANCE\")\nfor k in k_values:\n    print(f\"Current Value of K: {k}\")\n    steps = 50\n    pbar = tqdm(total=steps, desc=f\"Manhattan Distance Processing...\")\n    man_pred = k_fold_cross_validation(num_folds, k, sampled_train_x, sampled_train_y, \"manhattan\")\n    time.sleep(0.2)\n    pbar.update(1)\n    validation_labels_list = []\n    # print(f\"Progress on K value {k}: #\")\n\n    for cv_no in range(1, num_folds + 1):\n        # print(\"#\")\n        train_x, train_y, val_x, val_y, val_idx = k_fold_split(num_folds, cv_no, sampled_train_x, sampled_train_y)\n        # print(\"#\")\n        val_labels = sampled_train_y[val_idx]\n        # print(\"#\")\n        validation_labels_list.append(val_labels)\n        # print(\"#\")\n\n    validation_labels = cp.concatenate(validation_labels_list)\n    # print(\"...Done\")\n\n    print(f\"\\nK value (current): {k} for Manhattan DISTANCE\\n\")\n    accuracy_list_manhattan[k], f1_list_manhattan[k] = evaluate(man_pred, validation_labels)\n\nprint(\"Euclidean DISTANCE\")\nfor k in k_values:\n    print(f\"\\nK value (current): {k} for Euclidean DISTANCE\\n\")\n    euc_pred = k_fold_cross_validation(num_folds, k, sampled_train_x, sampled_train_y, \"euclidean\")\n    validation_labels_list = []\n    for cv_no in range(1, num_folds + 1):\n        train_x, train_y, val_x, val_y, val_idx = k_fold_split(num_folds, cv_no, sampled_train_x, sampled_train_y)\n        val_labels = sampled_train_y[val_idx]\n        validation_labels_list.append(val_labels)\n\n    validation_labels = cp.concatenate(validation_labels_list)\n    accuracy_list_euclidean[k], f1_list_euclidean[k] = evaluate(euc_pred, validation_labels)","metadata":{"id":"qwLHAHXyxx0B","outputId":"410b7e89-80e5-42aa-be83-9db094be904f","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:58:48.561793Z","iopub.execute_input":"2025-03-11T08:58:48.562135Z","iopub.status.idle":"2025-03-11T09:03:44.257011Z","shell.execute_reply.started":"2025-03-11T08:58:48.562073Z","shell.execute_reply":"2025-03-11T09:03:44.256186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, present the results as a graph with `k` values on the x-axis and classification accuracy on the y-axis. Use a single plot to compare the two versions of the classifier (one using Euclidean and the other using Manhattan distance metric).\n\nMake another graph but with the F1-score on the y-axis this time. The graphs should be properly labeled on axes, with a title, and a legend.\n","metadata":{"id":"gJJGuS9Uxx0B"}},{"cell_type":"code","source":"# TODO: Plot a graph with k values on the x-axis and classification accuracy on the y-axis (both distances on one plot).\nclass EuclideanData:\n  def __init__(self, k, accuracy, f1):\n    self.k_vals = k\n    self.accuracy_vals = {}\n    self.f1_vals = {}\n    for k, v in accuracy.items():\n      numpy_array = v.get()\n      self.accuracy_vals[k] = numpy_array\n    for k, v in accuracy.items():\n      numpy_array = v.get()\n      self.f1_vals[k] = numpy_array\n\n  def accuracy_list(self):\n    ext = []\n    ext = list(self.accuracy_vals.values())\n    return ext\n\n  def f1_list(self):\n    ext = []\n    ext = list(self.f1_vals.values())\n    return ext\n\nclass ManhattanData:\n  def __init__(self, k, accuracy, f1):\n    self.k_vals = k\n    self.accuracy_vals = {}\n    self.f1_vals = {}\n    for k, v in accuracy.items():\n      numpy_array = v.get()\n      self.accuracy_vals[k] = numpy_array\n    for k, v in accuracy.items():\n      numpy_array = v.get()\n      self.f1_vals[k] = numpy_array\n\n  def accuracy_list(self):\n    ext = []\n    ext = list(self.accuracy_vals.values())\n    return ext\n\n  def f1_list(self):\n    ext = []\n    ext = list(self.f1_vals.values())\n    return ext\n\neuclidean_data = EuclideanData(k_values, accuracy_list_euclidean, f1_list_euclidean)\nmanhattan_data = ManhattanData(k_values, accuracy_list_manhattan, f1_list_manhattan)\n\nplt.plot(euclidean_data.k_vals, euclidean_data.accuracy_list(), label='Euclidean Distance Accuracy', marker='o')\nplt.plot(manhattan_data.k_vals, manhattan_data.accuracy_list(), label='Manhattan Distance Accuracy', marker='x')\n\nplt.xlabel('K Values')\nplt.ylabel('Accuracy')\nplt.title('Accuracy Comparison b/w using Euclidean VS Manhattan Distance')\n\nplt.legend()\nplt.show()","metadata":{"id":"iiqoMt7qxx0C","outputId":"45eb1ff4-67c7-4cf7-f414-930af0badf59","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T09:03:50.225636Z","iopub.execute_input":"2025-03-11T09:03:50.225919Z","iopub.status.idle":"2025-03-11T09:03:50.428370Z","shell.execute_reply.started":"2025-03-11T09:03:50.225896Z","shell.execute_reply":"2025-03-11T09:03:50.427529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: Plot a graph with k values on the x-axis and F1-score on the y-axis (both distances on one plot).\nplt.plot(euclidean_data.k_vals, euclidean_data.f1_list(), label='Euclidean Distance F1-Score', marker='o')\nplt.plot(manhattan_data.k_vals, manhattan_data.f1_list(), label='Manhattan Distance F1-Score', marker='x')\n\nplt.xlabel('K Values')\nplt.ylabel('F1-Score')\nplt.title('F1-Score Comparison b/w using Euclidean VS Manhattan Distance')\n\nplt.legend()\nplt.show()","metadata":{"id":"5QSsKOEUxx0C","outputId":"43878e6f-1ed3-493f-bc76-b980eaf97c8c","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T09:03:57.539196Z","iopub.execute_input":"2025-03-11T09:03:57.539484Z","iopub.status.idle":"2025-03-11T09:03:57.736665Z","shell.execute_reply.started":"2025-03-11T09:03:57.539462Z","shell.execute_reply":"2025-03-11T09:03:57.735867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 1.6: Prediction\n\nFinally, use the best value of `k` for both distance metrics and run it on the test dataset.\n\nFind the confusion matrix, classification accuracy and F1 score and print them.\n\nThe confusion matrix must be displayed as a heatmap annotated with the data values. The axes should be properly labelled and the colormap used needs to be shown next to the heatmap.\n","metadata":{"id":"TbSxgYUKxx0D"}},{"cell_type":"code","source":"# TODO: Test with the best K for euclidean distance\nbest_k = None\ndef findBestKvalue(accuracy_list_euclidean,f1_list_euclidean,accuracy_list_manhattan,f1_list_manhattan):\n  best_k_acc_euc = max(accuracy_list_euclidean, key=accuracy_list_euclidean.get)\n  best_k_f1_euc = max(f1_list_euclidean, key=f1_list_euclidean.get)\n  best_k_acc_man = max(accuracy_list_manhattan, key=accuracy_list_manhattan.get)\n  best_k_f1_man = max(f1_list_manhattan, key=f1_list_manhattan.get)\n  result = max(best_k_acc_euc,best_k_f1_euc,best_k_acc_man,best_k_f1_man)\n  return result\n\nbest_k = findBestKvalue(accuracy_list_euclidean,f1_list_euclidean,accuracy_list_manhattan,f1_list_manhattan)\n# print(best_k)\nprint(f\"Best K: {best_k}\\n\")\nprint(\"Running KNN Model on test Data for Euclidean distance\")\nmodel = KNN(best_k)\nmodel.fit(train_x, train_y)\nprint(\"Starting Prediction process...\")\npredictions = model.predict(test_x, \"euclidean\")\n\naccuracy_euc, f1_euc = evaluate(predictions, test_y)\n","metadata":{"id":"qutQdgLKxx0D","outputId":"075a37b0-0491-40a1-a405-1d46690c75e0","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T09:04:03.934555Z","iopub.execute_input":"2025-03-11T09:04:03.934845Z","iopub.status.idle":"2025-03-11T09:04:25.417889Z","shell.execute_reply.started":"2025-03-11T09:04:03.934823Z","shell.execute_reply":"2025-03-11T09:04:25.416996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: Test with the best K for Manhattan distance\n# best_k = ...\nprint(\"Running KNN Model on test Data for Manhattan distance\")\nmodel = KNN(best_k)\nmodel.fit(train_x, train_y)\nprint(\"Starting Prediction process...\")\npredictions = model.predict(test_x, \"manhattan\")\n\naccuracy_manhattan, f1_manhattan = evaluate(predictions, test_y)\n\n","metadata":{"id":"bjqMxMsmxx0D","trusted":true,"execution":{"execution_failed":"2025-03-10T06:53:39.853Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 2: Decision Tree (350 Marks)\n\nA decision tree is a flowchart-like structure in which each internal node represents a test on a feature. Each leaf node represents a class label. The paths from the root to leaf represent classification rules.\n\nUse entropy as the measure of impurity and calculate the information gain to split the nodes.\n","metadata":{"id":"sMcLdkSexx0D"}},{"cell_type":"markdown","source":"### Importing necessary libraries for Part 2\n","metadata":{"id":"IRQxhwIqxx0E"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm \nimport seaborn as sns\nimport cupy as cp\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score","metadata":{"id":"gY_RFohExx0E","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:43:07.550964Z","iopub.execute_input":"2025-03-11T08:43:07.551216Z","iopub.status.idle":"2025-03-11T08:43:11.038483Z","shell.execute_reply.started":"2025-03-11T08:43:07.551192Z","shell.execute_reply":"2025-03-11T08:43:11.037446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preparing The Dataset for Part 2\n","metadata":{"id":"yL17qzxgxx0E"}},{"cell_type":"markdown","source":"### Task 2.1: Loading the dataset\n\nRead the dataset provided in the file `data.csv` using the pandas library.\n\n1. First, print the number of rows and columns (shape) in the dataset.\n2. Print the first 5 rows of the dataset.\n","metadata":{"id":"2knGDEVfxx0F"}},{"cell_type":"code","source":"# TODO: Load the dataset from the csv file using pandas. The file is called 'data.csv'\ndata = pd.read_csv('/kaggle/input/pa2-part2-ai/data.csv')\n# TODO: Print the shape of the data\nprint(f\"Shape of data: {data.shape}\\n\")\n# TODO: Print the first 5 rows of the data\nprint(\"First 5 rows of the data:\")\nprint(data.head())","metadata":{"id":"NjiQAySbxx0F","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:44:16.099156Z","iopub.execute_input":"2025-03-11T08:44:16.099461Z","iopub.status.idle":"2025-03-11T08:44:16.145185Z","shell.execute_reply.started":"2025-03-11T08:44:16.099438Z","shell.execute_reply":"2025-03-11T08:44:16.144343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2.2: Visualizing the dataset\n\nPlot a scatter plot of the data. Be sure to label the axes, give the plot a title, and include a legend.\n","metadata":{"id":"cC7BK30Jxx0F"}},{"cell_type":"code","source":"# TODO: Visualize the data using a scatter plot. Be sure to include a legend\nax = data.plot(kind='scatter', x='x1', y='x2', color='firebrick', label='Dataset points', s = 10, marker = '*')\n\n# Adding a title and axis labels\nax.set_title('Scatter Plot of x1 vs x2')  # Title of the plot\nax.set_xlabel('x1')  # Label for the x-axis\nax.set_ylabel('x2')  # Label for the y-axis\n\n# Adding a legend\nax.legend()\n\n# Show the plot\nplt.show()","metadata":{"id":"HD-w5C_Axx0G","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:44:26.401353Z","iopub.execute_input":"2025-03-11T08:44:26.401633Z","iopub.status.idle":"2025-03-11T08:44:26.742222Z","shell.execute_reply.started":"2025-03-11T08:44:26.401612Z","shell.execute_reply":"2025-03-11T08:44:26.741398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2.3: Splitting the dataset into train and test sets\n\nSplit the dataset into a training set and a testing set. You should have 80% of the data in the training set and 20% of the data in the test set. You are **not** allowed to use any library to do this.\n\nAfter splitting, print the number of rows in the training set and the test set.\n","metadata":{"id":"gIE12t7Lxx0G"}},{"cell_type":"code","source":"# TODO: split the dataset into train and test sets without using sklearn\n# use a 80/20 split\nsplit_index = int(0.8 * len(data))\ntrain_dat_temp = data[:split_index]\ntest_dat_temp = data[split_index:]\nrow,col = data.shape\n# TODO: print the number of rows in the train and test sets\n\n# Calculate the number of rows for training and testing sets\ntotal_rows = row\ntrain_size = train_dat_temp.size\ntest_size = test_dat_temp.size\n\n# Split the dataset into training and testing sets\ntrain_set = train_dat_temp\ntest_set = test_dat_temp\n\n# Print the number of rows in the training and test sets\nprint(\"Number of rows in the training set:\", len(train_set))\nprint(\"Number of rows in the test set:\", len(test_set))\n\nprint(train_set)\nprint(test_set)","metadata":{"id":"LevMYnAXxx0Q","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:44:32.059758Z","iopub.execute_input":"2025-03-11T08:44:32.060048Z","iopub.status.idle":"2025-03-11T08:44:32.074379Z","shell.execute_reply.started":"2025-03-11T08:44:32.060026Z","shell.execute_reply":"2025-03-11T08:44:32.073640Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Separate the features and the target variable in the training and testing set into `X_train`, `y_train`, `X_test`, and `y_test`.\n","metadata":{"id":"OOhFdnTqxx0R"}},{"cell_type":"code","source":"# TODO: Separate into X and Y\nX_train = train_set[['x1','x2']]\ny_train = train_set[\"y\"]\nX_test = test_set[['x1','x2']]\ny_test = test_set['y']","metadata":{"id":"ZPJ5zSwpxx0R","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:44:38.034591Z","iopub.execute_input":"2025-03-11T08:44:38.034875Z","iopub.status.idle":"2025-03-11T08:44:38.041672Z","shell.execute_reply.started":"2025-03-11T08:44:38.034852Z","shell.execute_reply":"2025-03-11T08:44:38.040549Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Use the following `plot_decision_boundary` function for plotting decision boundaries in Part 2 of the assignment\n","metadata":{"id":"BE3btakqxx0R"}},{"cell_type":"code","source":"# Do not modify the code below\n\ndef plot_decision_boundary(model, X, y):\n    '''\n    Function to plot the decision boundary of a classification model.\n    Parameters:\n        model: the classification model\n        X: the input features\n        y: the target labels\n    '''\n\n    # Set min and max values and give it some padding\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    h = 0.05\n\n    # Generate a grid of points with distance h between them\n    x1, x2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n                         np.arange(x2_min, x2_max, h))\n\n    # Predict the function value for the whole grid\n    Z = model.predict(np.c_[x1.ravel(), x2.ravel()])\n    \n    # Check if the result is a list and convert to a CuPy array if necessary\n    if isinstance(Z, list):\n        Z = cp.array(Z)  # Convert the list to CuPy array\n    \n    # Now convert the CuPy array to a NumPy array\n    Z = Z.get()  # Convert CuPy array to NumPy array\n    Z = Z.reshape(x1.shape)\n\n    # Plot the contour and training examples\n    plt.contourf(x1, x2, Z, cmap=plt.cm.Set1, alpha=0.4)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='.',\n                s=20, linewidth=1, alpha=0.2, cmap=plt.cm.Set1)\n    plt.xlabel(\"x1\")\n    plt.ylabel(\"x2\")\n    plt.title(\"Dataset\")\n    plt.show()\n","metadata":{"id":"QOEwl6LNxx0R","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:44:45.535226Z","iopub.execute_input":"2025-03-11T08:44:45.535558Z","iopub.status.idle":"2025-03-11T08:44:45.542760Z","shell.execute_reply.started":"2025-03-11T08:44:45.535529Z","shell.execute_reply":"2025-03-11T08:44:45.541855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2.4: Decision Tree Classifier\n\nThis class will serve as the decision tree classifier. It should have the following methods:\n\n- **init**: This is the constructor of the class. It initializes the class with the maximum depth of the tree. The default value of the maximum depth should be 5, but the user should be able to change it.\n- **fit**: This method takes the training data and the training labels as arguments. It should build the decision tree using the training data and labels. You are to use the algorithm discussed in the class to build the decision tree.\n- **predict**: This method takes the test data and returns the predicted labels for the test data.\n\nFeel free to add any other methods that you think might be useful.\n","metadata":{"id":"HVGGXq87xx0S"}},{"cell_type":"code","source":"# TODO: Build a decision tree classifier from scratch\n# NOTE: You are allowed to alter method signatures and create as many helper functions as needed\n# NOTE: You can also use a completely different implementation than what is stated below\n\nclass Node:\n    def __init__(self, left=None, right=None, feature=None, threshold=None, value=None):\n        self.left = left\n        self.right = right\n        self.feature = feature\n        self.threshold = threshold\n        self.value = value\n\n    def is_leaf(self):\n        return self.value is not None\n\n\nclass DecisionTree:\n    def __init__(self, max_depth=5, min_samples_splits=2, n_features=None):\n        self.max_depth = max_depth\n        self.min_samples_splits = min_samples_splits\n        self.n_features = n_features\n        self.root = None\n\n    def fit(self, X, y):\n        if not self.n_features:\n            self.n_features = X.shape[1]\n        \n        # with tqdm(total=self.max_depth, desc=\"Building decision tree\", position=0, leave=False) as pbar:\n            pbar = 0\n            self.root = self.make_tree(X, y, depth=0, pbar=pbar)\n\n    def predict(self, X_test):\n        return [self.traverse_tree(x, self.root) for x in X_test]\n\n    def traverse_tree(self, data_point, node):\n        if node.is_leaf():\n            return node.value\n        if data_point[node.feature] <= node.threshold:\n            return self.traverse_tree(data_point, node.left)\n        else:\n            return self.traverse_tree(data_point, node.right)\n\n    def make_tree(self, X, y, depth=0, pbar=None):\n        n_samples, n_feats = X.shape\n        \n        unique_labels = np.unique(y).size\n        \n        if depth >= self.max_depth or unique_labels == 1 or n_samples < self.min_samples_splits:\n            label = self.most_common_label(y)\n            return Node(value=label)\n    \n        feature_ids = np.random.choice(n_feats, self.n_features, replace=False)\n    \n        best_feat, best_thresh = self.best_split(X, y, feature_ids, pbar)\n    \n        left_indices, right_indices = self.split(X[:, best_feat], best_thresh)\n        \n        left = self.make_tree(X[left_indices], y[left_indices], depth + 1, pbar)\n        right = self.make_tree(X[right_indices], y[right_indices], depth + 1, pbar)\n    \n        return Node(left=left, right=right, feature=best_feat, threshold=best_thresh)\n\n    def most_common_label(self, y):\n        unique_labels, counts = np.unique(y, return_counts=True)\n        most_common_index = np.argmax(counts)\n        return unique_labels[most_common_index]\n\n    def best_split(self, X, y, feature_ids, pbar=None):\n        best_gain = -1\n        best_feature = None\n        best_threshold = None\n\n        for feature in tqdm(feature_ids, desc=\"Finding best split\", position=1, leave=False):\n            feature_values = X[:, feature]\n            thresholds = np.unique(feature_values)\n\n            for threshold in thresholds:\n                gain = self.calculate_information_gain(y, feature_values, threshold)\n\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = feature\n                    best_threshold = threshold\n\n        return best_feature, best_threshold\n\n    def split(self, feature_column, threshold):\n        left_indices = np.where(feature_column <= threshold)[0]\n        right_indices = np.where(feature_column > threshold)[0]\n        return left_indices, right_indices\n\n    def calculate_entropy(self, y):\n        y = cp.asarray(y).astype(cp.int32) \n        y_np = y.get() \n        \n        class_counts = np.bincount(y_np)\n        probabilities = class_counts / len(y_np)\n        \n        non_zero_probs = probabilities[probabilities > 0]\n        entropy = -np.sum(non_zero_probs * np.log(non_zero_probs))\n        \n        return entropy\n\n\n    def calculate_information_gain(self, y, feature_column, threshold):\n        parent_entropy = self.calculate_entropy(y)\n        \n        left_indices, right_indices = self.split(feature_column, threshold)\n        \n        if len(left_indices) == 0 or len(right_indices) == 0:\n            return 0\n        \n        left_entropy = self.calculate_entropy(y[left_indices])\n        right_entropy = self.calculate_entropy(y[right_indices])\n        \n        weighted_child_entropy = (len(left_indices) / len(y)) * left_entropy + (len(right_indices) / len(y)) * right_entropy\n\n        return parent_entropy - weighted_child_entropy\n\n","metadata":{"id":"kNhTqjw5xx0S","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:44:51.042571Z","iopub.execute_input":"2025-03-11T08:44:51.042917Z","iopub.status.idle":"2025-03-11T08:44:51.059790Z","shell.execute_reply.started":"2025-03-11T08:44:51.042887Z","shell.execute_reply":"2025-03-11T08:44:51.058980Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2.5: Evaluation\n\nEvaluate the Decision Tree classifier using the testing set. Print the accuracy, precision, recall, and F1 score of the classifier just like you did for the KNN classifier. Also, plot the confusion matrix of the classifier, as a heatmap.\n\nYou can also import and use pprint to print and visualize the decision tree. Although it is not mandatory, it can be useful to understand the decision tree better.\n","metadata":{"id":"J90peShhxx0S"}},{"cell_type":"code","source":"\n\n# Create a DecisionTree object and train it using the training data\n\n\n# Convert data to CuPy arrays\n\n\n\nX_train_cp = cp.asarray(X_train.to_numpy())\ny_train_cp = cp.asarray(y_train.to_numpy())\nX_test_cp = cp.asarray(X_test.to_numpy())\ny_test_cp = cp.asarray(y_test.to_numpy())\n\n# Fit the DecisionTree model\ndt = DecisionTree()\ndt.fit(X_train_cp, y_train_cp)\ndt_y_pred = dt.predict(X_test_cp)\n\n# Define the evaluation metrics using scikit-learn\n\n\nmetrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\nresults = {}\n\n\nwith tqdm(total=len(metrics), desc=\"Calculating Metrics\", position=0, leave=True) as pbar:\n    \n    y_test_np = y_test.to_numpy() \n    dt_y_pred_np = cp.asarray(dt_y_pred).get()  \n    accuracy = accuracy_score(y_test_np, dt_y_pred_np)\n    results[\"Accuracy\"] = accuracy\n    pbar.set_postfix({\"Accuracy\": f\"{accuracy:.2f}\"}) \n    pbar.update(1)  \n    \n    precision = precision_score(y_test_np, dt_y_pred_np)\n    results[\"Precision\"] = precision\n    pbar.set_postfix({\"Precision\": f\"{precision:.2f}\"})  \n    pbar.update(1) \n\n    recall = recall_score(y_test_np, dt_y_pred_np)\n    results[\"Recall\"] = recall\n    pbar.set_postfix({\"Recall\": f\"{recall:.2f}\"}) \n    pbar.update(1) \n    f1 = f1_score(y_test_np, dt_y_pred_np)\n    results[\"F1 Score\"] = f1\n    pbar.set_postfix({\"F1 Score\": f\"{f1:.2f}\"})  \n    pbar.update(1)\n\n# Print the results\nprint(f'Accuracy: {float(accuracy):.2f}')\nprint(f'Precision: {float(precision):.2f}')\nprint(f'Recall: {float(recall):.2f}')\nprint(f'F1 Score: {float(f1):.2f}')\n\n","metadata":{"id":"cDZLHSGHxx0T","outputId":"4015da98-b717-45cb-ff85-00bde1281c23","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:44:58.438842Z","iopub.execute_input":"2025-03-11T08:44:58.439146Z","iopub.status.idle":"2025-03-11T08:45:14.933874Z","shell.execute_reply.started":"2025-03-11T08:44:58.439121Z","shell.execute_reply":"2025-03-11T08:45:14.932831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: Create a confusion matrix and plot it using a heatmap.\n# You can use sklearn's confusion_matrix function to create the matrix\n# and seaborn's heatmap function to plot it. The libraries has been imported for you\n\nconf_matrix = confusion_matrix(y_test_np, dt_y_pred_np) \nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, cmap='Reds', fmt='d', cbar=True)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"id":"karwBzlkxx0T","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:45:24.757587Z","iopub.execute_input":"2025-03-11T08:45:24.757893Z","iopub.status.idle":"2025-03-11T08:45:24.943242Z","shell.execute_reply.started":"2025-03-11T08:45:24.757870Z","shell.execute_reply":"2025-03-11T08:45:24.942369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2.6: Plot Decision Boundary\n\nSimilar to the KNN classifier, plot the decision boundary for the decision tree classifier. Also, plot the decision boundary along with a scatter plot of the **Entire Data**. You are supposed to use the `plot_decision_boundary` function given to you at the start\n","metadata":{"id":"DLK3VP1Xxx0T"}},{"cell_type":"code","source":"# TODO: Plot the decision boundary on the training set\nX_all_np = cp.concatenate([X_train_cp, X_test_cp], axis=0).get() \ny_all_np = cp.concatenate([y_train_cp, y_test_cp], axis=0).get() \nplot_decision_boundary(dt, X_all_np, y_all_np)  \n","metadata":{"id":"hZwDOKocxx0U","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:45:30.712061Z","iopub.execute_input":"2025-03-11T08:45:30.712457Z","iopub.status.idle":"2025-03-11T08:45:35.096015Z","shell.execute_reply.started":"2025-03-11T08:45:30.712429Z","shell.execute_reply":"2025-03-11T08:45:35.095167Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Task 2.7: Different Max depth values\n\nAfter seeing how our initial model performed with a default value of Max depth , we will now explore different Max depth values. This step will help us find the best Max depth setting to improve our model's ability to predict accurately\n\n- Use the `plot_decision_boundary` function given above to plot the decision boundaries and scatter plots for different values of `max_depth` from 1 to 25.\n- Gather the accuracy for each `max_depth` value on the test set and compare them. Show these comparisons in a graph to easily see which `max_depth` value leads to the best predictions. This will help us choose the best k value for our model.\n","metadata":{"id":"0DKQg4Ruxx0U"}},{"cell_type":"code","source":"# TODO: Plot the decision boundary on the test set for different values of max_depth from 1 to 25\n\n\nmodel_accuracies = []\n\nwith tqdm(total=25, desc=\"Training decision trees\", position=0, leave=True) as pbar:\n    for max_depth in range(1, 26):\n        tree = DecisionTree(max_depth=max_depth)\n        tree.fit(X_train_cp, y_train_cp) \n        \n        dt_y_pred = tree.predict(X_test_cp)\n    \n        accuracy = accuracy_score(y_test_cp.get(), cp.asarray(dt_y_pred).get())  \n        \n \n        model_accuracies.append(accuracy) \n        pbar.set_postfix({\"Max Depth\": max_depth, \"Accuracy\": f\"{accuracy:.2f}\"})\n        pbar.update(1)  \n\n        X_test_np = X_test_cp.get()  \n        y_test_np = y_test_cp.get()  \n        print(f\"Decision Boundary for max_depth = {max_depth}\")\n        plot_decision_boundary(tree, X_test_np, y_test_np)  \n        # plt.title(f\"Decision Boundary for max_depth = {max_depth}\")\n        # plt.show()\n\n\nfor i, acc in enumerate(model_accuracies, start=1):\n    print(f\"Accuracy for max_depth {i}: {acc}\")\n    ","metadata":{"id":"ZVZo_yHxxx0U","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:45:42.117769Z","iopub.execute_input":"2025-03-11T08:45:42.118188Z","iopub.status.idle":"2025-03-11T08:55:37.991855Z","shell.execute_reply.started":"2025-03-11T08:45:42.118149Z","shell.execute_reply":"2025-03-11T08:55:37.991200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: Plot the accuracies of the model for different values of max_depth\nmax_depth_values = list(range(1, 26))\nx = max_depth_values \ny = model_accuracies  \n\nplt.figure(figsize=(8, 6))  \nplt.plot(x, y, label='Sine Wave', color='r', linewidth=2)\n\nplt.title('Accuracies of the model for different values of max_depth', fontsize=14, fontweight='bold')\nplt.xlabel('Max Depth Values)', fontsize=12)\nplt.ylabel('Model Accuracies', fontsize=12)\n\nplt.grid(True)\n\nplt.legend()\nplt.show()","metadata":{"id":"Bf7C1fyzxx0U","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T08:56:23.294994Z","iopub.execute_input":"2025-03-11T08:56:23.295302Z","iopub.status.idle":"2025-03-11T08:56:23.483374Z","shell.execute_reply.started":"2025-03-11T08:56:23.295279Z","shell.execute_reply":"2025-03-11T08:56:23.482475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Q. What can you conclude from you results? Explain your observations.\n","metadata":{"id":"p80IuAIBxx0V"}},{"cell_type":"markdown","source":"Ans.\n","metadata":{"id":"2GMP5pzXxx0V"}},{"cell_type":"markdown","source":"# Part 3: Naive Bayes (250 Marks)\n\n### Introduction\n\nIn this part, you will be implementing Naive Bayes model based on the dataset features and task requirements.\n\nFor reference and additional details, please go through [Chapter 4](https://web.stanford.edu/~jurafsky/slp3/) of the SLP3 book.\n\nIn this assignment, you are provided with `golf_data.csv`. Your task is to:\n\n1. Analyze the datasets and the dataset’s characteristics.\n2. Implement both **Naive Bayes** from scratch, adhering to the guidelines below regarding allowed libraries.\n3. Finally, apply the corresponding models using the `sklearn` library and compare the results with your own implementation.\n\n### Guidelines:\n\n- Use only **numpy** and **pandas** for the manual implementation of Naive Bayes classifier. No other libraries should be used for this part.\n- For the final part of the assignment, you will use **sklearn** to compare your implementation results.\n","metadata":{"id":"d7hL-3Moxx0V"}},{"cell_type":"markdown","source":"All necessary libraries for this assignment have already been added. You are not allowed to add any additional imports.\n","metadata":{"id":"M9i90no4xx0W"}},{"cell_type":"code","source":"# Standard library imports\nimport numpy as np\n\n# Third-party library imports\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\nfrom sklearn.naive_bayes import BernoulliNB\nfrom tqdm.notebook import tqdm","metadata":{"id":"xTPr_ALGxx0W","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T15:38:59.904922Z","iopub.execute_input":"2025-03-11T15:38:59.905307Z","iopub.status.idle":"2025-03-11T15:39:00.118744Z","shell.execute_reply.started":"2025-03-11T15:38:59.905278Z","shell.execute_reply":"2025-03-11T15:39:00.117681Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## Task 3.1: Loading the Datasets\n\nIn this assignment, you are provided with the dataset:\n\n- Golf Dataset (available in CSV format in the given folder)\n\n### Instructions:\n\n**Golf Dataset**: You can find the CSV file of the Golf Dataset in the resources provided with this assignment. This dataset aims to explore factors that influence the decision to play golf, which could be valuable for predictive modeling tasks. ​​\n\nYou can read more about Bernoulli Naive Bayes [here](https://medium.com/@gridflowai/part-2-dive-into-bernoulli-naive-bayes-d0cbcbabb775).\n","metadata":{"id":"hYdSKHpExx0W"}},{"cell_type":"code","source":"# code here\ngolf_data = pd.read_csv('/kaggle/input/part3-ai-ml-pa2/golf_data.csv')  # Replace with correct path\ngolf_data.info()","metadata":{"id":"xl1Ao5uWxx0X","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T15:39:03.070407Z","iopub.execute_input":"2025-03-11T15:39:03.070745Z","iopub.status.idle":"2025-03-11T15:39:03.104392Z","shell.execute_reply.started":"2025-03-11T15:39:03.070719Z","shell.execute_reply":"2025-03-11T15:39:03.103442Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7665 entries, 0 to 7664\nData columns (total 9 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   Holiday      7665 non-null   int64 \n 1   Month        7665 non-null   object\n 2   Season       7665 non-null   object\n 3   Temperature  7665 non-null   object\n 4   Humidity     7665 non-null   object\n 5   Windy        7665 non-null   int64 \n 6   Outlook      7665 non-null   object\n 7   Crowdedness  7665 non-null   object\n 8   Play         7665 non-null   int64 \ndtypes: int64(3), object(6)\nmemory usage: 539.1+ KB\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## Task 3.2: Data Preprocessing\n","metadata":{"id":"K50B_IjCxx0X"}},{"cell_type":"markdown","source":"### Preprocessing the Golf Dataset\n","metadata":{"id":"t_UkNSwwxx0X"}},{"cell_type":"markdown","source":"In this task, you will apply one-hot encoding to the categorical columns of the Golf dataset (you can use `pd's` `get_dummies`) and split the data into training and test sets. You can use `sklearn's` `train_test_split` which has been imported for you above. Ensure that the `test_size` parameter is set to 0.3.\n","metadata":{"id":"GFtcspyKxx0X"}},{"cell_type":"code","source":"# code here\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\nclass PreprocessData:\n    def __init__(self):\n        self.data_frame = None\n        self.x = None\n        self.y = None\n        self.x_train = None\n        self.y_train = None\n        self.x_test = None\n        self.y_test = None\n\n    def fit(self, dataset):\n        self.data_frame = dataset.copy()\n        self.encode_cols()\n        self.extract_x_y()\n\n    def encode_cols(self):\n        for c in self.data_frame.columns:\n            unique_options = self.data_frame[c].unique()\n            mapping = {unique_options[0]: 0, unique_options[1]: 1}\n            self.data_frame[c] = self.data_frame[c].map(mapping)\n    \n    def extract_x_y(self):\n        self.y = self.data_frame['Play']\n        self.x = self.data_frame.drop('Play', axis=1)\n\n    def split(self, test_size=0.3, random_state=42):\n        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x, self.y, test_size=test_size, random_state=random_state)\n        return self.x_train, self.x_test, self.y_train, self.y_test\n\n    def print_one_hotted_set(self):\n        print(\"The dataset with one hot encoding!\")\n        print(self.data_frame)\n\n\npp = PreprocessData()\npp.fit(golf_data)\nx_train, x_test, y_train, y_test = pp.split()\n","metadata":{"id":"VDDQLn51xx0X","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T15:39:05.982720Z","iopub.execute_input":"2025-03-11T15:39:05.983124Z","iopub.status.idle":"2025-03-11T15:39:06.008431Z","shell.execute_reply.started":"2025-03-11T15:39:05.983053Z","shell.execute_reply":"2025-03-11T15:39:06.007298Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## Task 3.3: Implementing Naive Bayes from Scratch\n","metadata":{"id":"usQfMuw9xx0Y"}},{"cell_type":"markdown","source":"## Bernoulli Naive Bayes\n\n### From Scratch\n\nRecall that the Bernoulli Naive Bayes model is based on **Bayes' Theorem**:\n\n$$\nP(y \\mid x) = \\frac{P(x \\mid y)P(y)}{P(x)}\n$$\n\nWhat we really want is to find the class \\(c\\) that maximizes \\(P(c \\mid x)\\), so we can use the following equation:\n\n$$\n\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(x \\mid c)P(c)\n$$\n\nIn the case of **Bernoulli Naive Bayes**, we assume that each word \\(x_i\\) in a sentence follows a **Bernoulli distribution**, meaning that the word either appears (1) or does not appear (0) in the document. We can simplify the formula using this assumption:\n\n$$\n\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c) \\prod_{i=1}^{n} P(x_i = 1 \\mid c)^{x_i} P(x_i = 0 \\mid c)^{1 - x_i}\n$$\n\nWhere:\n\n- $x_i = 1$ if the $i^{\\text{th}}$ word is present in the document.\n- $x_i = 0$ if the $i^{\\text{th}}$ word is not present in the document.\n\nWe can estimate $P(c)$ by counting the number of times each class appears in our training data, and dividing by the total number of training examples. We can estimate $P(x_i = 1 \\mid c)$ by counting the number of documents in class $c$ that contain the word $x_i$, and dividing by the total number of documents in class $c$.\n\n### **Important: Laplace Smoothing**\n\nWhen calculating $P(x_i = 1 \\mid c)$ and $P(x_i = 0 \\mid c)$, we apply **Laplace smoothing** to avoid zero probabilities. This is essential because, without it, any word that has not appeared in a document of class $c$ will have a probability of zero, which would make the overall product zero, leading to incorrect classification.\n\n**Reason**: Laplace smoothing ensures that we don't encounter zero probabilities by adding a small constant (typically 1) to both the numerator and the denominator. This is particularly useful when a word has never appeared in the training data for a specific class.\n\nThe smoothed probability formula is:\n\n$$\nP(x_i = 1 \\mid c) = \\frac{\\text{count of documents in class } c \\text{ where } x_i = 1 + 1}{\\text{total documents in class } c + 2}\n$$\n\nThis ensures no word has a zero probability, even if it was unseen in the training data.\n\n### Avoiding Underflow with Logarithms:\n\nTo avoid underflow errors due to multiplying small probabilities, we apply logarithms, which convert the product into a sum:\n\n$$\n\\hat{c} = \\underset{c}{\\text{argmax}} \\ \\log P(c) + \\sum_{i=1}^{n} \\left[ x_i \\log P(x_i = 1 \\mid c) + (1 - x_i) \\log P(x_i = 0 \\mid c) \\right]\n$$\n\nYou will now implement this algorithm.\n\n<span style=\"color: red;\"> For this part, the only external library you will need is `numpy`. You are not allowed to use anything else.</span>\n","metadata":{"id":"tImUGscuxx0Y"}},{"cell_type":"code","source":"class BNB:\n    def __init__(self, X):\n        self.cond_probs = {}      \n        self.feature_priors1 = {}     # will store P(x_i = 1 | c) for each class c\n        self.feature_priors0 = {}     # will store P(x_i = 0 | c) for each class c\n        self.classes = [1, 0]\n        self.class_counts = {}\n        self.priors = {}\n        self.samples, self.n_features = X.shape\n        self.X = None\n        self.y = None\n        self.feature_prob = {}        # stores probability for x_i = 1 given class c\n    \n    # p(y) helper: Calculate class priors P(c)\n    def class_prior_calculator(self):\n        count_yes = np.sum(self.y == 1)\n        count_no = np.sum(self.y == 0)\n        self.class_counts[1] = count_yes\n        self.class_counts[0] = count_no\n        self.priors[1] = count_yes / self.samples\n        self.priors[0] = count_no / self.samples\n        \n    def feature_prior_calculator(self):\n        for c in tqdm(self.classes, desc='Feature Calculation Progress:'):\n            X_c = self.X[self.y == c]\n            count = np.sum(X_c, axis=0)\n            # Apply Laplace smoothing: (count + 1)/(total in class c + 2)\n            prob = (count + 1) / (self.class_counts[c] + 2)\n            self.feature_prob[c] = prob\n            self.feature_priors1[c] = prob\n            self.feature_priors0[c] = 1 - prob\n\n    def model_fit(self, X, y):\n        self.X = X\n        self.y = y\n        self.samples, self.n_features = X.shape\n        self.class_prior_calculator()\n        self.feature_prior_calculator()\n\n    def model_predict(self, test_x):\n        test_x = np.array(test_x)\n        n_samples, n_features = test_x.shape\n        predictions = []\n        for i in tqdm(range(n_samples), desc='Predicting:'):\n            x_i = test_x[i] \n            log_probs = {}\n            for c in self.classes:\n                log_prob = np.log(self.priors[c])\n                prob_feat = self.feature_prob[c]\n                log_prob += np.sum(x_i * np.log(prob_feat) + (1 - x_i) * np.log(1 - prob_feat))\n                log_probs[c] = log_prob\n            best_class = max(log_probs, key=log_probs.get)\n            predictions.append(best_class)\n        return np.array(predictions)\n","metadata":{"id":"ZlxPYqoexx0Y","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T15:40:47.727752Z","iopub.execute_input":"2025-03-11T15:40:47.728172Z","iopub.status.idle":"2025-03-11T15:40:47.740021Z","shell.execute_reply.started":"2025-03-11T15:40:47.728140Z","shell.execute_reply":"2025-03-11T15:40:47.738867Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"Now use your implementation to train a Naive Bayes model on the training data, and generate predictions for the Validation Set.\n\nReport the Accuracy, Precision, Recall, and F1 score of your model on the validation data. Also display the Confusion Matrix. You are allowed to use `sklearn.metrics` for this.\n","metadata":{"id":"tzT5pv-Vxx0Z"}},{"cell_type":"code","source":"# code here\nnaive = BNB(x_train)\nnaive.model_fit(x_train,y_train)\ny_pred = naive.model_predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, pos_label=1) \nrecall = recall_score(y_test, y_pred, pos_label=1)\nf1 = f1_score(y_test, y_pred, pos_label=1)\nreport = classification_report(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\nprint(\"\\nClassification Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n","metadata":{"id":"7YHnubrxxx0Z","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T15:40:49.924862Z","iopub.execute_input":"2025-03-11T15:40:49.925270Z","iopub.status.idle":"2025-03-11T15:40:52.705667Z","shell.execute_reply.started":"2025-03-11T15:40:49.925227Z","shell.execute_reply":"2025-03-11T15:40:52.704200Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Feature Calculation Progress::   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a988e82c1d549e9862e66b65cbe9f36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Predicting::   0%|          | 0/2300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf18b57ef7aa4a87b25f62b1aceda163"}},"metadata":{}},{"name":"stdout","text":"Accuracy: 0.82\nPrecision: 0.8235811702595689\nRecall: 0.993103448275862\nF1 Score: 0.9004329004329006\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.52      0.03      0.06       415\n           1       0.82      0.99      0.90      1885\n\n    accuracy                           0.82      2300\n   macro avg       0.67      0.51      0.48      2300\nweighted avg       0.77      0.82      0.75      2300\n\nConfusion Matrix:\n [[  14  401]\n [  13 1872]]\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## WELL DONE! You have successfully completed the Assignment! 🎉💯\n","metadata":{"id":"W9eQnQjpxx0Z"}}]}